# Build and Test Instructions for Batch Size Optimization

## Summary of Changes

Added batch size optimization to improve prompt processing performance for large PDFs:
- **Files Modified**: `src/commands.cpp`, `src/delta_server_wrapper.cpp`
- **Performance Improvement**: Expected ~4x faster (683s â†’ ~170s for 20k token prompts)

## Build Instructions

### Prerequisites

1. **CMake** (version 3.14+)
2. **C++17 compatible compiler**
3. **llama.cpp submodule** (should already be present)

### Build Steps

#### macOS

```bash
# Navigate to project root
cd /Users/suzanodero/io/GITHUB/delta

# Create build directory (if not exists)
mkdir -p build_macos
cd build_macos

# Configure with CMake
cmake ..

# Build
make -j$(sysctl -n hw.ncpu)

# The binaries will be in build_macos/
# - delta (main CLI)
# - delta-server (server wrapper)
```

#### Linux

```bash
cd /Users/suzanodero/io/GITHUB/delta

mkdir -p build_linux
cd build_linux

cmake ..

make -j$(nproc)
```

#### Windows

```powershell
cd C:\path\to\delta

mkdir build_windows
cd build_windows

cmake ..

cmake --build . --config Release
```

## Testing the Fix

### Step 1: Stop Current Delta Server

If Delta is currently running, stop it:

```bash
# Find and kill existing delta-server processes
pkill -f delta-server
# or
pkill -f llama-server
```

### Step 2: Start Delta with New Build

```bash
# From build directory
./delta --server --port 8080

# Or if installed system-wide
delta --server --port 8080
```

### Step 3: Verify Batch Size Parameters

Check that llama-server is started with the new batch size parameters:

```bash
# Check running processes
ps aux | grep llama-server

# You should see something like:
# llama-server ... --ubatch-size 2048 --batch-size 4096 ...
```

**Note**: The batch size parameters are only added if `ctx_size >= 8192`. Check your model's context size.

### Step 4: Test with Large PDF

1. **Open Delta Web UI**: `http://localhost:8080`

2. **Upload the same PDF** that previously took 683 seconds

3. **Send a message** with the PDF attached

4. **Monitor Processing Time**:
   - Check browser console for timing information
   - Look for `prompt_progress` updates
   - Expected: ~170-175 seconds (vs previous 683 seconds)

### Step 5: Verify Performance Improvement

Compare the results:

| Metric | Before | After (Expected) | Improvement |
|--------|--------|------------------|-------------|
| Processing Time | 683.63s | ~170-175s | ~4x faster |
| Tokens/Second | ~30 | ~117-120 | ~4x faster |

## Troubleshooting

### Issue: Batch size parameters not appearing

**Check context size**:
- Batch sizes are only set if `ctx_size >= 4096`
- For `ctx_size >= 8192`: `--ubatch-size 2048 --batch-size 4096`
- For `ctx_size >= 4096`: `--ubatch-size 1024 --batch-size 2048`

**Verify model context size**:
```bash
# Check model registry or model info
delta --models
```

### Issue: Still slow processing

1. **Verify llama-server version**: Ensure you're using a recent version that supports `--ubatch-size`
2. **Check system resources**: Large batch sizes require more memory
3. **Monitor CPU/GPU usage**: Should see higher utilization with larger batches

### Issue: Build errors

1. **Clean build**:
   ```bash
   rm -rf build_macos
   mkdir build_macos
   cd build_macos
   cmake ..
   make -j
   ```

2. **Check submodules**:
   ```bash
   git submodule update --init --recursive
   ```

## Verification Checklist

- [ ] Code compiles without errors
- [ ] Delta server starts successfully
- [ ] llama-server process shows batch size parameters (for ctx_size >= 4096)
- [ ] Large PDF processing time reduced from ~683s to ~170-175s
- [ ] No regressions for small prompts (<4096 context)

## Rollback Instructions

If needed, revert the changes:

```bash
git checkout src/commands.cpp src/delta_server_wrapper.cpp
```

Then rebuild.

## Next Steps

After verifying the fix works:

1. **Commit the changes**:
   ```bash
   git add src/commands.cpp src/delta_server_wrapper.cpp
   git commit -m "Optimize batch sizes for large prompt processing

   - Add --ubatch-size and --batch-size parameters based on context size
   - Improves prompt processing speed by ~4x for large prompts (20k+ tokens)
   - Matches LlamaBarn's performance optimization approach"
   ```

2. **Test with different PDF sizes** to ensure no regressions

3. **Monitor production usage** to verify performance improvements
